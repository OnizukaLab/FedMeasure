{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append('./utils/')\n",
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to experiment with a different seed value, change 'trial_times'\n",
    "trial_times = 1\n",
    "SEED = 42 + trial_times -1\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 20\n",
    "    self.test_batch = 1000\n",
    "    #self.global_epochs = 300\n",
    "    self.local_epochs = 100\n",
    "    self.lr = 10**(-3)\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 100\n",
    "    self.worker_num = 20\n",
    "    self.participation_rate = 1\n",
    "    self.sample_num = int(self.worker_num * self.participation_rate)\n",
    "    self.total_data_rate = 1\n",
    "    self.unlabeleddata_size = 1000\n",
    "    self.device = device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    ## If you use MNIST or CIFAR-10, the degree of data heterogeneity can be changed by changing alpha_label and alpha_size.\n",
    "    self.alpha_label = 0.5\n",
    "    self.alpha_size = 10\n",
    "    \n",
    "    ## Select a dataset from 'FEMNIST','Shakespeare','Sent140','MNIST', or 'CIFAR-10'.\n",
    "    self.dataset_name = 'FEMNIST'\n",
    "\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_name=='FEMNIST':\n",
    "    from femnist_dataset import *\n",
    "    args.num_classes = 62\n",
    "    model_name = \"CNN(num_classes=args.num_classes)\"\n",
    "    \n",
    "elif args.dataset_name=='Shakespeare':\n",
    "    from shakespeare_dataset import *\n",
    "    model_name = \"RNN()\"\n",
    "    \n",
    "elif args.dataset_name=='Sent140':\n",
    "    from sent140_dataset import *\n",
    "    from utils_sent140 import *\n",
    "    VOCAB_DIR = '../models/embs.json'\n",
    "    _, indd, vocab = get_word_emb_arr(VOCAB_DIR)\n",
    "    model_name = \"RNNSent(args,'LSTM', 2, 25, 128, 1, 0.5, tie_weights=False)\"\n",
    "    \n",
    "elif args.dataset_name=='MNIST':\n",
    "    from mnist_dataset import *\n",
    "    args.num_classes = 10\n",
    "    model_name = \"CNN(num_classes=args.num_classes)\"\n",
    "    \n",
    "elif args.dataset_name=='CIFAR-10':\n",
    "    from cifar10_dataset import *\n",
    "    model_name = \"vgg13()\"\n",
    "    \n",
    "else:\n",
    "    print('Error: The name of the dataset is incorrect. Please re-set the \"dataset_name\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_trainset,federated_valset,federated_testset,unlabeled_dataset = get_dataset(args, unlabeled_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self):\n",
    "    self.model = eval(model_name)\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(federated_trainset[i],federated_valset[i],federated_testset[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "\n",
    "    for worker in workers:\n",
    "      worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "      worker.model = copy.deepcopy(self.model)\n",
    "      worker.model = worker.model.to(args.device)\n",
    "\n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = OrderedDict()\n",
    "    for i,worker in enumerate(workers):\n",
    "      worker_state = worker.model.state_dict()\n",
    "      for key in worker_state.keys():\n",
    "        if i==0:\n",
    "          new_params[key] = worker_state[key]*worker.aggregation_weight\n",
    "        else:\n",
    "          new_params[key] += worker_state[key]*worker.aggregation_weight\n",
    "      worker.model = worker.model.to('cpu')\n",
    "      del worker.model\n",
    "    self.model.load_state_dict(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,trainset,valset,testset):\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.model = None\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "    self.aggregation_weight = None\n",
    "\n",
    "  def local_train(self):\n",
    "    acc_valid,loss_valid = train(self.model,args.criterion,self.trainloader,self.valloader,args.local_epochs,partience=args.partience,early_stop=True)\n",
    "    return acc_valid[-1],loss_valid[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,valloader,epochs,partience=0,early_stop=False):\n",
    "  if args.dataset_name=='Sent140':\n",
    "      if early_stop:\n",
    "        early_stopping = Early_Stopping(partience)\n",
    "\n",
    "      acc_valid = []\n",
    "      loss_valid = []\n",
    "      optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "      hidden_train = model.init_hidden(args.batch_size)\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        model.train()\n",
    "        for (data,labels) in trainloader:\n",
    "          data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "          if args.batch_size != 1 and data.shape[0] != args.batch_size:\n",
    "            break\n",
    "          data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "          optimizer.zero_grad()\n",
    "          hidden_train = repackage_hidden(hidden_train)\n",
    "          outputs, hidden_train = model(data, hidden_train) \n",
    "          loss = criterion(outputs.t(), torch.max(labels, 1)[1])\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "          optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        hidden_test = model.init_hidden(args.test_batch)\n",
    "        for (data,labels) in valloader:\n",
    "          data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "          if args.test_batch != 1 and data.shape[0] != args.test_batch:\n",
    "            break\n",
    "          data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "          hidden_test = repackage_hidden(hidden_test)\n",
    "          outputs, hidden_test = model(data, hidden_test) \n",
    "          running_loss += criterion(outputs.t(), torch.max(labels, 1)[1]).item()\n",
    "          _, predicted = torch.max(outputs.t(), 1)\n",
    "          correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "          count += len(labels)\n",
    "\n",
    "        print('Epoch:{}  accuracy:{}  loss:{}'.format(epoch+1,100.0*correct/count,running_loss/len(valloader)))\n",
    "        acc_valid.append(100.0*correct/count)\n",
    "        loss_valid.append(running_loss/len(valloader))\n",
    "        if early_stop:\n",
    "          if early_stopping.validate(running_loss):\n",
    "            print('Early Stop')\n",
    "            return acc_valid,loss_valid\n",
    "\n",
    "      return acc_valid,loss_valid\n",
    "\n",
    "  else:\n",
    "      if early_stop:\n",
    "        early_stopping = Early_Stopping(partience)\n",
    "\n",
    "      acc_valid = []\n",
    "      loss_valid = []\n",
    "      optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        model.train()\n",
    "        for (data,labels) in trainloader:\n",
    "          data,labels = Variable(data),Variable(labels)\n",
    "          data,labels = data.to(args.device),labels.to(args.device)\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(data)\n",
    "          loss = criterion(outputs,labels)\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "          optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        for (data,labels) in valloader:\n",
    "          count += len(labels)\n",
    "          data,labels = data.to(args.device),labels.to(args.device)\n",
    "          outputs = model(data)\n",
    "          loss = criterion(outputs,labels)\n",
    "          running_loss += loss.item()\n",
    "          predicted = torch.argmax(outputs,dim=1)\n",
    "          correct += (predicted==labels).sum().item()\n",
    "\n",
    "        print('Epoch:{}  accuracy:{}  loss:{}'.format(epoch+1,100.0*correct/count,running_loss/len(valloader)))\n",
    "        acc_valid.append(100.0*correct/count)\n",
    "        loss_valid.append(running_loss/len(valloader))\n",
    "        if early_stop:\n",
    "          if early_stopping.validate(running_loss):\n",
    "            print('Early Stop')\n",
    "            return acc_valid,loss_valid\n",
    "\n",
    "      return acc_valid,loss_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  if args.dataset_name=='Sent140':\n",
    "      model.eval()\n",
    "      hidden_test = model.init_hidden(args.test_batch)\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in testloader:\n",
    "        data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "        if args.test_batch != 1 and data.shape[0] != args.test_batch:\n",
    "          break\n",
    "        data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "        hidden_test = repackage_hidden(hidden_test)\n",
    "        outputs, hidden_test = model(data, hidden_test) \n",
    "        running_loss += criterion(outputs.t(), torch.max(labels, 1)[1]).item()\n",
    "        _, predicted = torch.max(outputs.t(), 1)\n",
    "        correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "        count += len(labels)\n",
    "\n",
    "      accuracy = 100.0*correct/count\n",
    "      loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "      return accuracy,loss\n",
    "\n",
    "  else:\n",
    "      model.eval()\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in testloader:\n",
    "        data,labels = data.to(args.device),labels.to(args.device)\n",
    "        outputs = model(data)\n",
    "        running_loss += criterion(outputs,labels).item()\n",
    "        predicted = torch.argmax(outputs,dim=1)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "        count += len(labels)\n",
    "\n",
    "      accuracy = 100.0*correct/count\n",
    "      loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "      return accuracy,loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "-noG_98IR-nZ",
    "outputId": "78a6ebe2-854a-4f83-dc45-5c4ac35b69e8"
   },
   "outputs": [],
   "source": [
    "server = Server()\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "    print('Worker{} start'.format(i+1))\n",
    "    worker.model = eval(model_name)\n",
    "    worker.model = worker.model.to(args.device)\n",
    "    acc_tmp,loss_tmp = worker.local_train()\n",
    "    acc_valid.append(acc_tmp)\n",
    "    loss_valid.append(loss_tmp)\n",
    "    worker.model = worker.model.to('cpu')\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train timeï¼š{}[s]'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "    worker.model = worker.model.to(args.device)\n",
    "    acc_tmp,loss_tmp = test(worker.model,args.criterion,worker.testloader)\n",
    "    acc_test.append(acc_tmp)\n",
    "    loss_test.append(loss_tmp)\n",
    "    print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    worker.model = worker.model.to('cpu')\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_valid_avg = sum(acc_valid)/len(acc_valid)\n",
    "loss_valid_avg = sum(loss_valid)/len(loss_valid)\n",
    "print('Validation  loss:{}  accuracy:{}'.format(loss_valid_avg,acc_valid_avg))\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Local_Data_Only_{}'.format(args.dataset_name)\n",
    "result_path = '../result/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_valid = pd.DataFrame(acc_valid)\n",
    "loss_valid = pd.DataFrame(loss_valid)\n",
    "\n",
    "acc_test = pd.DataFrame(acc_test)\n",
    "loss_test = pd.DataFrame(loss_test)\n",
    "\n",
    "\n",
    "acc_valid.to_csv(result_path+filename+'_valid_acc.csv',index=False, header=False)\n",
    "loss_valid.to_csv(result_path+filename+'_valid_loss.csv',index=False, header=False)\n",
    "acc_test.to_csv(result_path+filename+'_test_acc.csv',index=False, header=False)\n",
    "loss_test.to_csv(result_path+filename+'_test_loss.csv',index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
