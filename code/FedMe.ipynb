{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append('./utils/')\n",
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to experiment with a different seed value, change 'trial_times'\n",
    "trial_times = 1\n",
    "SEED = 42 + trial_times -1\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 20 \n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 300\n",
    "    self.local_epochs = 2\n",
    "    self.lr = 10**(-3)\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 300\n",
    "    self.worker_num = 20\n",
    "    self.participation_rate = 1\n",
    "    self.sample_num = int(self.worker_num * self.participation_rate)\n",
    "    self.total_data_rate = 1\n",
    "    self.cluster_list = [1,2,3,4]\n",
    "    self.cluster_num = None\n",
    "    self.turn_of_cluster_num = [0,150,225,275]\n",
    "    self.turn_of_replacement_model = list(range(self.global_epochs))\n",
    "    self.unlabeleddata_size = 1000\n",
    "    self.device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion_ce = nn.CrossEntropyLoss()\n",
    "    self.criterion_kl = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    ## If you use MNIST or CIFAR-10, the degree of data heterogeneity can be changed by changing alpha_label and alpha_size.\n",
    "    self.alpha_label = 0.5\n",
    "    self.alpha_size = 10\n",
    "    \n",
    "    ## Select a dataset from 'FEMNIST','Shakespeare','Sent140','MNIST', or 'CIFAR-10'.\n",
    "    self.dataset_name = 'FEMNIST'\n",
    "\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_name=='FEMNIST':\n",
    "    from femnist_dataset import *\n",
    "    args.num_classes = 62\n",
    "    model_name = \"CNN(num_classes=args.num_classes)\"\n",
    "    \n",
    "elif args.dataset_name=='Shakespeare':\n",
    "    from shakespeare_dataset import *\n",
    "    model_name = \"RNN()\"\n",
    "    \n",
    "elif args.dataset_name=='Sent140':\n",
    "    from sent140_dataset import *\n",
    "    from utils_sent140 import *\n",
    "    VOCAB_DIR = '../models/embs.json'\n",
    "    _, indd, vocab = get_word_emb_arr(VOCAB_DIR)\n",
    "    model_name = \"RNNSent(args,'LSTM', 2, 25, 128, 1, 0.5, tie_weights=False)\"\n",
    "    \n",
    "elif args.dataset_name=='MNIST':\n",
    "    from mnist_dataset import *\n",
    "    args.num_classes = 10\n",
    "    model_name = \"CNN(num_classes=args.num_classes)\"\n",
    "    \n",
    "elif args.dataset_name=='CIFAR-10':\n",
    "    from cifar10_dataset import *\n",
    "    model_name = \"vgg13()\"\n",
    "    \n",
    "else:\n",
    "    print('Error: The name of the dataset is incorrect. Please re-set the \"dataset_name\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_trainset,federated_valset,federated_testset,unlabeled_dataset = get_dataset(args, unlabeled_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self,unlabeled_dataset):\n",
    "    self.cluster = None\n",
    "    self.models = None\n",
    "    self.unlabeled_dataloader = torch.utils.data.DataLoader(unlabeled_dataset,batch_size=args.batch_size,shuffle=False,num_workers=2)\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(i,federated_trainset[i],federated_valset[i],federated_testset[i],))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "  def collect_model(self,workers):\n",
    "    self.models = [None]*args.worker_num\n",
    "    for worker in workers:\n",
    "      self.models[worker.id] = copy.deepcopy(worker.local_model)\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    for worker in workers:\n",
    "      worker.local_model = copy.deepcopy(self.models[worker.id])\n",
    "      worker.other_model = copy.deepcopy(self.models[worker.other_model_id])\n",
    "        \n",
    "  def return_model(self,workers):\n",
    "    for worker in workers:\n",
    "      worker.local_model = copy.deepcopy(self.models[worker.local_model_id])\n",
    "      worker.local_model_id = worker.id\n",
    "    del self.models\n",
    "    \n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = []\n",
    "    train_model_id = []\n",
    "    train_model_id_count = []\n",
    "    for worker in workers:\n",
    "      worker_state = worker.local_model.state_dict()\n",
    "      if worker.id in train_model_id:\n",
    "        i = train_model_id.index(worker.id)\n",
    "        for key in worker_state.keys():\n",
    "          new_params[i][key] += worker_state[key]\n",
    "        train_model_id_count[i] += 1\n",
    "      else:\n",
    "        new_params.append(OrderedDict())\n",
    "        train_model_id.append(worker.id)\n",
    "        train_model_id_count.append(1)\n",
    "        i = train_model_id.index(worker.id)\n",
    "        for key in worker_state.keys():\n",
    "          new_params[i][key] = worker_state[key]\n",
    "        \n",
    "      worker_state = worker.other_model.state_dict()\n",
    "      if worker.other_model_id in train_model_id:\n",
    "        i = train_model_id.index(worker.other_model_id)\n",
    "        for key in worker_state.keys():\n",
    "          new_params[i][key] += worker_state[key]\n",
    "        train_model_id_count[i] += 1\n",
    "      else:\n",
    "        new_params.append(OrderedDict())\n",
    "        train_model_id.append(worker.other_model_id)\n",
    "        train_model_id_count.append(1)\n",
    "        i = train_model_id.index(worker.other_model_id)\n",
    "        for key in worker_state.keys():\n",
    "          new_params[i][key] = worker_state[key]\n",
    "        \n",
    "      worker.local_model = worker.local_model.to('cpu')\n",
    "      worker.other_model = worker.other_model.to('cpu')\n",
    "      del worker.local_model,worker.other_model\n",
    "    \n",
    "    for i,model_id in enumerate(train_model_id):\n",
    "      for key in new_params[i].keys():\n",
    "        new_params[i][key] = new_params[i][key]/train_model_id_count[i]\n",
    "      self.models[model_id].load_state_dict(new_params[i])\n",
    "      \n",
    "  '''clustering by kmeans'''  \n",
    "  def clustering(self,workers):\n",
    "    if args.cluster_num==1:\n",
    "        pred = [0]*len(workers)\n",
    "        worker_id_list = []\n",
    "        for worker in workers:\n",
    "            worker_id_list.append(worker.id)\n",
    "    else:\n",
    "        if args.dataset_name=='Sent140':\n",
    "            with torch.no_grad():\n",
    "                worker_softmax_targets = [[] for i in range(len(workers))]\n",
    "                worker_id_list = []\n",
    "                count = 0\n",
    "                for i,model in enumerate(self.models):\n",
    "                  if model==None:\n",
    "                    pass\n",
    "                  else:\n",
    "                    model = model.to(args.device)\n",
    "                    model.eval()\n",
    "                    hidden_test = model.init_hidden(args.batch_size)\n",
    "                    for data,_ in self.unlabeled_dataloader:\n",
    "                      data = process_x(data, indd)\n",
    "                      if args.batch_size != 1 and data.shape[0] != args.batch_size:\n",
    "                        break\n",
    "                      data = torch.from_numpy(data).to(args.device)\n",
    "                      hidden_test = repackage_hidden(hidden_test)\n",
    "                      outputs, hidden_test = model(data, hidden_test)\n",
    "                      worker_softmax_targets[count].append(outputs.to('cpu').detach().numpy())\n",
    "                    worker_softmax_targets[count] = np.array(worker_softmax_targets[count])\n",
    "                    model = model.to('cpu')\n",
    "                    worker_id_list.append(i)\n",
    "                    count += 1\n",
    "                worker_softmax_targets = np.array(worker_softmax_targets)\n",
    "                kmeans = KMeans(n_clusters=args.cluster_num)\n",
    "                pred = kmeans.fit_predict(worker_softmax_targets)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                worker_softmax_targets = [[] for i in range(len(workers))]\n",
    "                worker_id_list = []\n",
    "                count = 0\n",
    "                for i,model in enumerate(self.models):\n",
    "                  if model==None:\n",
    "                    pass\n",
    "                  else:\n",
    "                    model = model.to(args.device)\n",
    "                    model.eval()\n",
    "                    for data,_ in self.unlabeled_dataloader:\n",
    "                      data = data.to(args.device)\n",
    "                      worker_softmax_targets[count].append(model(data).to('cpu').detach().numpy())\n",
    "                    worker_softmax_targets[count] = np.array(worker_softmax_targets[count])\n",
    "                    model = model.to('cpu')\n",
    "                    worker_id_list.append(i)\n",
    "                    count += 1\n",
    "                worker_softmax_targets = np.array(worker_softmax_targets)\n",
    "                kmeans = KMeans(n_clusters=args.cluster_num)\n",
    "                pred = kmeans.fit_predict(worker_softmax_targets)\n",
    "            \n",
    "    self.cluster = []\n",
    "    for i in range(args.cluster_num):\n",
    "      self.cluster.append([])\n",
    "    for i,cls in enumerate(pred):\n",
    "      self.cluster[cls].append(worker_id_list[i])\n",
    "    for worker in workers:\n",
    "      idx = worker_id_list.index(worker.id)\n",
    "      worker.cluster_num = pred[idx]\n",
    "        \n",
    "  def decide_other_model(self,workers):\n",
    "    for worker in workers:\n",
    "      cls = worker.cluster_num\n",
    "      '''if number of worker in cluster is one, other model is decided by random in all workers. '''\n",
    "      if len(self.cluster[cls])==1:\n",
    "        while True:\n",
    "          other_worker = random.choice(workers)\n",
    "          other_model_id = other_worker.id\n",
    "          if worker.id!=other_model_id:\n",
    "            break\n",
    "      else:\n",
    "        while True:\n",
    "          other_model_id = random.choice(self.cluster[cls])\n",
    "          if worker.id!=other_model_id:\n",
    "            break\n",
    "      worker.other_model_id = other_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,i,trainset,valset,testset):\n",
    "    self.id = i\n",
    "    self.cluster_num = None\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.local_model = eval(model_name)\n",
    "    self.local_model_id = i\n",
    "    self.other_model = None\n",
    "    self.other_model_id = None\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "\n",
    "  def local_train(self):\n",
    "    if args.dataset_name=='Sent140':\n",
    "        self.local_model.train()\n",
    "        self.other_model.train()\n",
    "        self.local_model = self.local_model.to(args.device)\n",
    "        self.other_model = self.other_model.to(args.device)\n",
    "        local_hidden_train = self.local_model.init_hidden(args.batch_size)\n",
    "        other_hidden_train = self.other_model.init_hidden(args.batch_size)\n",
    "        local_optimizer = optim.SGD(self.local_model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "        other_optimizer = optim.SGD(self.other_model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "        for epoch in range(args.local_epochs):\n",
    "          running_loss = 0.0\n",
    "          correct = 0\n",
    "          count = 0\n",
    "          for (data,labels) in self.trainloader:\n",
    "            data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "            if args.batch_size != 1 and data.shape[0] != args.batch_size:\n",
    "              break\n",
    "            data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device) \n",
    "            local_optimizer.zero_grad()\n",
    "            other_optimizer.zero_grad()\n",
    "            local_hidden_train = repackage_hidden(local_hidden_train)\n",
    "            other_hidden_train = repackage_hidden(other_hidden_train)\n",
    "            local_outputs, local_hidden_train = self.local_model(data, local_hidden_train) \n",
    "            other_outputs, other_hidden_train = self.other_model(data, other_hidden_train) \n",
    "\n",
    "            #train local_model\n",
    "            ce_loss = args.criterion_ce(local_outputs.t(), torch.max(labels, 1)[1])\n",
    "            kl_loss = args.criterion_kl(F.log_softmax(local_outputs, dim = 1),F.softmax(Variable(other_outputs), dim=1))\n",
    "            loss = ce_loss + kl_loss\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(local_outputs.t(), 1)\n",
    "            correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "            count += len(labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), args.clip)\n",
    "            local_optimizer.step()\n",
    "\n",
    "            #train other_model\n",
    "            ce_loss = args.criterion_ce(other_outputs.t(), torch.max(labels, 1)[1])\n",
    "            kl_loss = args.criterion_kl(F.log_softmax(other_outputs, dim = 1),F.softmax(Variable(local_outputs), dim=1))\n",
    "            loss = ce_loss + kl_loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.other_model.parameters(), args.clip)\n",
    "            other_optimizer.step()\n",
    "\n",
    "        return 100.0*correct/count,running_loss/len(self.trainloader)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        self.local_model = self.local_model.to(args.device)\n",
    "        self.other_model = self.other_model.to(args.device)\n",
    "        local_optimizer = optim.SGD(self.local_model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "        other_optimizer = optim.SGD(self.other_model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "        self.local_model.train()\n",
    "        self.other_model.train()\n",
    "        for epoch in range(args.local_epochs):\n",
    "          running_loss = 0.0\n",
    "          correct = 0\n",
    "          count = 0\n",
    "          for (data,labels) in self.trainloader:\n",
    "            data,labels = Variable(data),Variable(labels)\n",
    "            data,labels = data.to(args.device),labels.to(args.device)\n",
    "            local_optimizer.zero_grad()\n",
    "            other_optimizer.zero_grad()\n",
    "            local_outputs = self.local_model(data)\n",
    "            other_outputs = self.other_model(data)\n",
    "            #train local_model\n",
    "            ce_loss = args.criterion_ce(local_outputs,labels)\n",
    "            kl_loss = args.criterion_kl(F.log_softmax(local_outputs, dim = 1),F.softmax(Variable(other_outputs), dim=1))\n",
    "            loss = ce_loss + kl_loss\n",
    "            running_loss += loss.item()\n",
    "            predicted = torch.argmax(local_outputs,dim=1)\n",
    "            correct += (predicted==labels).sum().item()\n",
    "            count += len(labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), args.clip)\n",
    "            local_optimizer.step()\n",
    "\n",
    "            #train other_model\n",
    "            ce_loss = args.criterion_ce(other_outputs,labels)\n",
    "            kl_loss = args.criterion_kl(F.log_softmax(other_outputs, dim = 1),F.softmax(Variable(local_outputs), dim=1))\n",
    "            loss = ce_loss + kl_loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.other_model.parameters(), args.clip)\n",
    "            other_optimizer.step()\n",
    "\n",
    "        return 100.0*correct/count,running_loss/len(self.trainloader)        \n",
    "\n",
    "        \n",
    "  def validate(self):\n",
    "    acc,loss = test(self.local_model,args.criterion_ce,self.valloader)\n",
    "    return acc,loss\n",
    "\n",
    "\n",
    "  def model_replacement(self):\n",
    "    _,loss_local = test(self.local_model,args.criterion_ce,self.valloader)\n",
    "    _,loss_other = test(self.other_model,args.criterion_ce,self.valloader)\n",
    "    if loss_other<loss_local:\n",
    "        self.local_model_id = self.other_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,epochs):\n",
    "  if args.dataset_name=='Sent140':\n",
    "      model.train()\n",
    "      hidden_train = model.init_hidden(args.batch_size)\n",
    "      optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for (data,labels) in trainloader:\n",
    "          data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "          if args.batch_size != 1 and data.shape[0] != args.batch_size:\n",
    "            break\n",
    "          data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "          optimizer.zero_grad()\n",
    "          hidden_train = repackage_hidden(hidden_train)\n",
    "          outputs, hidden_train = model(data, hidden_train) \n",
    "          loss = criterion(outputs.t(), torch.max(labels, 1)[1])\n",
    "          running_loss += loss.item()\n",
    "          _, predicted = torch.max(outputs.t(), 1)\n",
    "          correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "          count += len(labels)\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "          optimizer.step()\n",
    "\n",
    "      return 100.0*correct/count,running_loss/len(trainloader)\n",
    "\n",
    "\n",
    "  else:\n",
    "      optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "      model.train()\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for (data,labels) in trainloader:\n",
    "          data,labels = Variable(data),Variable(labels)\n",
    "          data,labels = data.to(args.device),labels.to(args.device)\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(data)\n",
    "          loss = criterion(outputs,labels)\n",
    "          running_loss += loss.item()\n",
    "          predicted = torch.argmax(outputs,dim=1)\n",
    "          correct += (predicted==labels).sum().item()\n",
    "          count += len(labels)\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "          optimizer.step()\n",
    "\n",
    "      return 100.0*correct/count,running_loss/len(trainloader)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  if args.dataset_name=='Sent140':\n",
    "      model.eval()\n",
    "      hidden_test = model.init_hidden(args.test_batch)\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in testloader:\n",
    "        data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "        if args.test_batch != 1 and data.shape[0] != args.test_batch:\n",
    "          break\n",
    "        data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "        hidden_test = repackage_hidden(hidden_test)\n",
    "        outputs, hidden_test = model(data, hidden_test) \n",
    "        running_loss += criterion(outputs.t(), torch.max(labels, 1)[1]).item()\n",
    "        _, predicted = torch.max(outputs.t(), 1)\n",
    "        correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "        count += len(labels)\n",
    "\n",
    "      accuracy = 100.0*correct/count\n",
    "      loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "      return accuracy,loss\n",
    "\n",
    "\n",
    "  else:\n",
    "      model.eval()\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in testloader:\n",
    "        data,labels = data.to(args.device),labels.to(args.device)\n",
    "        outputs = model(data)\n",
    "        running_loss += criterion(outputs,labels).item()\n",
    "        predicted = torch.argmax(outputs,dim=1)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "        count += len(labels)\n",
    "\n",
    "      accuracy = 100.0*correct/count\n",
    "      loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "      return accuracy,loss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "-noG_98IR-nZ",
    "outputId": "78a6ebe2-854a-4f83-dc45-5c4ac35b69e8"
   },
   "outputs": [],
   "source": [
    "server = Server(unlabeled_dataset)\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(args.global_epochs):\n",
    "  if epoch in args.turn_of_cluster_num:\n",
    "    idx = args.turn_of_cluster_num.index(epoch)\n",
    "    args.cluster_num = args.cluster_list[idx]\n",
    "  sample_worker = server.sample_worker(workers)\n",
    "  server.collect_model(sample_worker)\n",
    "  server.clustering(sample_worker)\n",
    "  server.decide_other_model(sample_worker)\n",
    "  server.send_model(sample_worker)\n",
    "\n",
    "  acc_train_avg = 0.0\n",
    "  loss_train_avg = 0.0\n",
    "  acc_valid_avg = 0.0\n",
    "  loss_valid_avg = 0.0\n",
    "  for worker in sample_worker:\n",
    "    acc_train_tmp,loss_train_tmp = worker.local_train()\n",
    "    acc_valid_tmp,loss_valid_tmp = worker.validate()\n",
    "    acc_train_avg += acc_train_tmp/len(sample_worker)\n",
    "    loss_train_avg += loss_train_tmp/len(sample_worker)\n",
    "    acc_valid_avg += acc_valid_tmp/len(sample_worker)\n",
    "    loss_valid_avg += loss_valid_tmp/len(sample_worker)\n",
    "  if epoch in args.turn_of_replacement_model:\n",
    "    for worker in sample_worker:\n",
    "      worker.model_replacement()\n",
    "  server.aggregate_model(sample_worker)\n",
    "  server.return_model(sample_worker)\n",
    "  '''\n",
    "  server.model.to(args.device)\n",
    "  for worker in workers:\n",
    "    acc_valid_tmp,loss_valid_tmp = test(server.model,args.criterion,worker.valloader)\n",
    "    acc_valid_avg += acc_valid_tmp/len(workers)\n",
    "    loss_valid_avg += loss_valid_tmp/len(workers)\n",
    "  server.model.to('cpu')\n",
    "  '''\n",
    "  print('Epoch{}  loss:{}  accuracy:{}'.format(epoch+1,loss_valid_avg,acc_valid_avg))\n",
    "  acc_train.append(acc_train_avg)\n",
    "  loss_train.append(loss_train_avg)\n",
    "  acc_valid.append(acc_valid_avg)\n",
    "  loss_valid.append(loss_valid_avg)\n",
    "\n",
    "  if early_stopping.validate(loss_valid_avg):\n",
    "    print('Early Stop')\n",
    "    break\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train timeï¼š{}[s]'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.local_model = worker.local_model.to(args.device)\n",
    "  acc_tmp,loss_tmp = test(worker.local_model,args.criterion_ce,worker.testloader)\n",
    "  acc_test.append(acc_tmp)\n",
    "  loss_test.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "  worker.local_model = worker.local_model.to('cpu')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.local_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_tune_test = []\n",
    "loss_tune_test = []\n",
    "acc_tune_valid = []\n",
    "loss_tune_valid = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "    worker.local_model = worker.local_model.to(args.device)\n",
    "    _,_ = train(worker.local_model,args.criterion_ce,worker.trainloader,args.local_epochs)\n",
    "    acc_tmp,loss_tmp = test(worker.local_model,args.criterion_ce,worker.valloader)\n",
    "    acc_tune_valid.append(acc_tmp)\n",
    "    loss_tune_valid.append(loss_tmp)\n",
    "    print('Worker{} Valid accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    \n",
    "    acc_tmp,loss_tmp = test(worker.local_model,args.criterion_ce,worker.testloader)\n",
    "    acc_tune_test.append(acc_tmp)\n",
    "    loss_tune_test.append(loss_tmp)\n",
    "    print('Worker{} Test accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    worker.local_model = worker.local_model.to('cpu')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_valid_avg = sum(acc_tune_valid)/len(acc_tune_valid)\n",
    "loss_valid_avg = sum(loss_tune_valid)/len(loss_tune_valid)\n",
    "print('Validation(tune)  loss:{}  accuracy:{}'.format(loss_valid_avg,acc_valid_avg))\n",
    "acc_test_avg = sum(acc_tune_test)/len(acc_tune_test)\n",
    "loss_test_avg = sum(loss_tune_test)/len(loss_tune_test)\n",
    "print('Test(tune)  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'FedMe_{}'.format(args.dataset_name)\n",
    "result_path = '../result/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = pd.DataFrame(acc_train)\n",
    "loss_train = pd.DataFrame(loss_train)\n",
    "acc_valid = pd.DataFrame(acc_valid)\n",
    "loss_valid = pd.DataFrame(loss_valid)\n",
    "\n",
    "acc_test = pd.DataFrame(acc_test)\n",
    "loss_test = pd.DataFrame(loss_test)\n",
    "\n",
    "\n",
    "acc_train.to_csv(result_path+filename+'_train_acc.csv',index=False, header=False)\n",
    "loss_train.to_csv(result_path+filename+'_train_loss.csv',index=False, header=False)\n",
    "acc_valid.to_csv(result_path+filename+'_valid_acc.csv',index=False, header=False)\n",
    "loss_valid.to_csv(result_path+filename+'_valid_loss.csv',index=False, header=False)\n",
    "acc_test.to_csv(result_path+filename+'_test_acc.csv',index=False, header=False)\n",
    "loss_test.to_csv(result_path+filename+'_test_loss.csv',index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_tune_valid = pd.DataFrame(acc_tune_valid)\n",
    "loss_tune_valid = pd.DataFrame(loss_tune_valid)\n",
    "acc_tune_test = pd.DataFrame(acc_tune_test)\n",
    "loss_tune_test = pd.DataFrame(loss_tune_test)\n",
    "\n",
    "acc_tune_valid.to_csv(result_path+filename+'_fine-tune_valid_acc.csv',index=False, header=False)\n",
    "loss_tune_valid.to_csv(result_path+filename+'_fine-tune_valid_loss.csv',index=False, header=False)\n",
    "acc_tune_test.to_csv(result_path+filename+'_fine-tune_test_acc.csv',index=False, header=False)\n",
    "loss_tune_test.to_csv(result_path+filename+'_fine-tune_test_loss.csv',index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
